{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWZW-xzI7oIS"
   },
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('complaints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .msg files as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extract_msg\n",
    "# data_path = Path('folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern1 = re.compile(r'From: .*')\n",
    "# pattern2 = re.compile(r'Sent: .*')\n",
    "# pattern3 = re.compile(r'To: .*')\n",
    "# pattern4 = re.compile(r'[\\n\\r]')\n",
    "# pattern5 = re.compile(r'\\d+')\n",
    "# pattern6 = re.compile(r'\\S+@\\S+')\n",
    "# pattern7 = re.compile(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~]')\n",
    "# patterns = [pattern1, pattern2, pattern3, pattern4, pattern5, pattern6, pattern7]\n",
    "# pattern_a = re.compile(r'Cc: (.*)')\n",
    "\n",
    "# def remove_pattern(*patterns, text, group=0):\n",
    "#     '''\n",
    "#     :param patterns: list of patterns where each patter is a re.compile\n",
    "#     :param text: string, text\n",
    "#     :return: text, string, having all the patterns removed\n",
    "#     '''\n",
    "#     for pattern in patterns:\n",
    "#         matches = pattern.finditer(text)\n",
    "#         temp = ''\n",
    "#         pos_prev = 0\n",
    "#         pos_cur = 0\n",
    "#         for match in matches:\n",
    "#             pos_cur = match.span(0)[0]\n",
    "#             temp += text[pos_prev: pos_cur]\n",
    "#             temp += ' '\n",
    "#             pos_prev = match.span(0)[1]\n",
    "#         temp += text[pos_prev:]\n",
    "#         if temp != '':\n",
    "#             text = temp\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_names = os.listdir(data_path)\n",
    "\n",
    "# data = pd.DataFrame(columns=['file_name', 'message'], index=range(len(file_names)))\n",
    "# for i, file_name in enumerate(file_names):\n",
    "#     file_path = data_path / file_name\n",
    "#     msg = extract_msg.Message(file_path)\n",
    "#     msg_message = msg.body\n",
    "#     msg_message = remove_pattern(*[pattern_a], text=msg_message, group=1)\n",
    "#     msg_message = remove_pattern(*patterns, text=msg_message)\n",
    "#     data.iloc[i] = file_name, msg_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179776, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have outdated information on my credit repor...</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n",
       "      <td>Consumer Loan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Consumer complaint narrative           Product\n",
       "0  I have outdated information on my credit repor...  Credit reporting\n",
       "1  I purchased a new car on XXXX XXXX. The car de...     Consumer Loan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179776, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna(axis=0, subset=['Consumer complaint narrative']).reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt collection                                                                 43946\n",
       "Mortgage                                                                        34576\n",
       "Credit reporting                                                                31589\n",
       "Credit card                                                                     18845\n",
       "Bank account or service                                                         14892\n",
       "Student loan                                                                    12047\n",
       "Consumer Loan                                                                    9486\n",
       "Credit reporting, credit repair services, or other personal consumer reports     6118\n",
       "Payday loan                                                                      1750\n",
       "Credit card or prepaid card                                                      1518\n",
       "Money transfers                                                                  1496\n",
       "Prepaid card                                                                     1451\n",
       "Checking or savings account                                                       892\n",
       "Vehicle loan or lease                                                             352\n",
       "Other financial service                                                           290\n",
       "Payday loan, title loan, or personal loan                                         285\n",
       "Money transfer, virtual currency, or money service                                227\n",
       "Virtual currency                                                                   16\n",
       "Name: Product, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Product.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(cols, ratio, n):\n",
    "    global data\n",
    "    new_data = pd.DataFrame(columns=['Product', 'Consumer complaint narrative'])\n",
    "    for i, col in enumerate(cols):\n",
    "        a = int(n * ratio[i] / sum(ratio))\n",
    "        temp_data = data[data['Product'] == col].reset_index(drop=True)\n",
    "        for j in range(a):\n",
    "            new_data = new_data.append(temp_data.iloc[j, :])\n",
    "        new_data = new_data.iloc[np.random.choice(a=new_data.shape[0], size=new_data.shape[0], replace=False), :]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(cols=['Debt collection', 'Mortgage', 'Credit reporting'], ratio=[1, 1, 1], n=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>I lost my job XXXX 2016. Was on unemployment. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I went through a credit consolidation company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>We are subject yet again, to more delays from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>XXXX, Equifax, and XXXX placed fraud alerts on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Equifax is not responding to my request to inv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Product                       Consumer complaint narrative\n",
       "115          Mortgage  I lost my job XXXX 2016. Was on unemployment. ...\n",
       "105   Debt collection  I went through a credit consolidation company ...\n",
       "158          Mortgage  We are subject yet again, to more delays from ...\n",
       "86   Credit reporting  XXXX, Equifax, and XXXX placed fraud alerts on...\n",
       "70   Credit reporting  Equifax is not responding to my request to inv..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data['Product'].isna().sum())\n",
    "print(data['Consumer complaint narrative'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(data['Consumer complaint narrative'].values)\n",
    "y = list(data['Product'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# vectorizer.fit(x)\n",
    "# vocab = vectorizer.vocabulary_\n",
    "# vocab = dict(sorted(vocab.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On XX/XX/XXXX the state of XXXX filed a tax lien against me. This lien has been reflecting on my XXXX and Transunion credit reports for the past several years. \\n\\nI am an XXXX, between XX/XX/XXXX and XX/XX/XXXX I was XXXX in XXXX. During that time I had moved to XXXX different residences due to work. I did not receive notice of delinquent taxes during my XXXX in that state. Nor did I receive notification of delinquent taxes after moving to XXXX on XXXX in XX/XX/XXXX. \\n\\nAt this point I do not have tax documentation from XX/XX/XXXX through XX/XX/XXXX and will gladly pay the debt to satisfy the issue as it is unnecessarily damaging my credit. \\n\\nI never received proper notification of the debt, and am writing to kindly request the tax lien be withdrawn, and I be allowed the opportunity to pay the debt. In addition I ask that XXXX, and Transunion remove that negative information from my credit report. \\n\\nI have contacted the XXXX State Department of Assessments & Taxation in an attempt to remedy the issue however was not successful. \\n\\nTax lien information : Case/Docket # XXXX Court : Circuit Court for XXXX County Court Address : XXXX Thank you in advance for any support you can provide.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    x[i] = re.sub(pattern, ' ', x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On XX/XX/XXXX the state of XXXX filed a tax lien against me. This lien has been reflecting on my XXXX and Transunion credit reports for the past several years.   I am an XXXX, between XX/XX/XXXX and XX/XX/XXXX I was XXXX in XXXX. During that time I had moved to XXXX different residences due to work. I did not receive notice of delinquent taxes during my XXXX in that state. Nor did I receive notification of delinquent taxes after moving to XXXX on XXXX in XX/XX/XXXX.   At this point I do not have tax documentation from XX/XX/XXXX through XX/XX/XXXX and will gladly pay the debt to satisfy the issue as it is unnecessarily damaging my credit.   I never received proper notification of the debt, and am writing to kindly request the tax lien be withdrawn, and I be allowed the opportunity to pay the debt. In addition I ask that XXXX, and Transunion remove that negative information from my credit report.   I have contacted the XXXX State Department of Assessments & Taxation in an attempt to remedy the issue however was not successful.   Tax lien information : Case/Docket # XXXX Court : Circuit Court for XXXX County Court Address : XXXX Thank you in advance for any support you can provide.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "15% of the total data is used as validation data while the remaining as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utterances: 450\n",
      "Validation utterances: 150\n"
     ]
    }
   ],
   "source": [
    "x_train_text, x_test_text, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=123)\n",
    "print('Training utterances: {}'.format(len(x_train_text)))\n",
    "print('Validation utterances: {}'.format(len(x_test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 2 0 0 2 1 0 1 2 2 2 2 0 2 2 0 1 0 0 2 2 1 0 0 1 1 1 0 1 2 1 1 1 1\n",
      " 1 0 0 2 0 0 0 2 0 2 2 1 1 1 1 2 0 1 2 1 0 2 0 1 1 0 0 2 1 0 2 2 2 0 2 0 1\n",
      " 0 0 1 1 2 1 0 0 0 1 0 1 2 2 2 2 1 1 1 0 1 0 1 0 2 2 1 2 1 0 2 0 0 2 1 0 0\n",
      " 0 2 1 0 2 2 0 2 0 1 2 0 1 2 0 1 2 1 1 2 2 1 1 2 1 0 1 0 0 0 0 0 2 2 1 2 0\n",
      " 1 2 0 2 0 0 2 0 0 1 0 0 1 1 1 0 1 2 1 1 2 0 1 2 0 0 1 1 0 2 2 0 2 2 1 2 2\n",
      " 2 0 0 1 2 1 1 0 2 0 1 0 0 1 2 0 2 2 2 0 0 0 2 1 1 2 0 0 2 2 0 2 0 1 0 0 1\n",
      " 1 1 2 2 2 0 0 2 2 1 1 1 1 0 0 1 2 0 2 1 2 0 1 2 1 1 1 0 2 1 1 2 0 0 0 2 1\n",
      " 1 1 2 2 2 1 0 2 0 1 0 2 0 0 2 0 0 0 1 1 1 2 0 0 1 2 0 1 1 2 2 1 2 0 0 0 1\n",
      " 1 0 0 0 0 2 2 1 1 2 0 0 1 0 1 0 0 1 0 2 1 0 2 1 1 0 1 2 2 2 1 2 2 0 0 0 2\n",
      " 1 1 0 2 1 0 1 2 0 2 1 1 2 0 0 1 2 2 1 1 1 0 2 2 0 1 0 2 0 0 2 2 2 0 0 0 2\n",
      " 0 2 1 1 2 0 2 1 1 1 0 2 1 0 1 1 0 2 2 2 2 1 2 2 1 2 1 0 0 1 0 2 1 1 2 1 0\n",
      " 1 2 0 0 2 1 2 2 2 1 2 0 1 1 1 1 2 1 2 1 2 2 0 0 0 2 0 2 0 2 2 0 1 1 1 0 1\n",
      " 0 0 2 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "print(y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDrAJxi-hd8O"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2961,
     "status": "ok",
     "timestamp": 1607801168326,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "hJqaHxkkMYU5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1607801168981,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "l5f2azRZBo8q",
    "outputId": "ffe5e691-1ba4-4351-ad5b-64737dd56434"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean([len(text.split()) for text in x_train_text])\n",
    "std = np.std([len(text.split()) for text in x_train_text])\n",
    "max_length = int(std + mean)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model data\n",
    "import pickle\n",
    "if not os.path.exists('model_data'):\n",
    "    os.mkdir('model_data')\n",
    "pickle.dump(le, open(Path('model_data') / 'le.pickle', 'wb'))\n",
    "pickle.dump(max_length, open(Path('model_data') / 'max_length.pickle', 'wb'))\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.mkdir('saved_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In case of Resource exhausted error, lower the batch_size, which might result in loss of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsfh7B49H5Yf"
   },
   "source": [
    "### DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1421,
     "status": "ok",
     "timestamp": 1607802142727,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "kfOhYPTuH7TB"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertConfig, DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2307,
     "status": "ok",
     "timestamp": 1607802143878,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "xrEcx2nDH7TB",
    "outputId": "2ecf36da-9caa-4441-c4f3-ceff14c80d38",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Name of the BERT model to use\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "# Load transformers config and set number of classes\n",
    "config = DistilBertConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_classes\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=config)\n",
    "\n",
    "# Load the base BERT model\n",
    "distilbert_model = TFDistilBertModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1573,
     "status": "ok",
     "timestamp": 1607802145615,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "oJygZh1SH7TC"
   },
   "outputs": [],
   "source": [
    "# Tokenize texts \n",
    "def tokenize(text):\n",
    "    tokenized = tokenizer(text,\n",
    "                          max_length=max_length,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          return_tensors='tf',\n",
    "                          return_token_type_ids=False,\n",
    "                          return_attention_mask=False,\n",
    "                          verbose=True)\n",
    "    return tokenized\n",
    "\n",
    "x_train_enc = tokenize(x_train_text)\n",
    "x_test_enc = tokenize(x_test_text)\n",
    "\n",
    "y_train_enc_one_hot = to_categorical(y_train_enc, num_classes=num_classes)\n",
    "y_test_enc_one_hot = to_categorical(y_test_enc, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2182,
     "status": "ok",
     "timestamp": 1607802148307,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "zuaInCwXH7TC",
    "outputId": "cbbdac5e-9785-4a18-8549-44338dc51ed6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Complaint_Classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 347)]             0         \n",
      "_________________________________________________________________\n",
      "distilbert (TFDistilBertMain ((None, 347, 768),)       66362880  \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           (None, 3)                 2307      \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define layers (head-layers)\n",
    "distilbert = distilbert_model.layers[0]\n",
    "pre_classifier = Dense(units=config.dim,\n",
    "                       kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "                       activation='relu',\n",
    "                       name='pre_classifier')\n",
    "classifier = Dense(units=config.num_labels,\n",
    "                   kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "                   name='classifier')\n",
    "dropout = Dropout(config.seq_classif_dropout)\n",
    "\n",
    "# Define model input\n",
    "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Forward pass\n",
    "distilbert_output = distilbert(inputs)\n",
    "hidden_state = distilbert_output[0]\n",
    "pooled_output = hidden_state[:, 0]\n",
    "pooled_output = pre_classifier(pooled_output)\n",
    "pooled_output = dropout(pooled_output) # , training=False True or False?\n",
    "logits = classifier(pooled_output) \n",
    "\n",
    "# complete a model using TF functional API\n",
    "model = Model(inputs=inputs, outputs=logits, name='Complaint_Classifier')\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1607802148310,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "7VAlqtlyb1Lu"
   },
   "outputs": [],
   "source": [
    "lr = 1e-05\n",
    "bs = 8\n",
    "# set callbacks\n",
    "cb = [ModelCheckpoint(filepath='saved_models/'+'model.{epoch:02d}-{accuracy:02f}-{val_accuracy:.2f}',\n",
    "                      monitor='val_accuracy',\n",
    "                      mode='max',\n",
    "                      save_best_only=True)] # callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - ETA: 0s - loss: 1.0671 - accuracy: 0.4489WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc13e150>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0d7f10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0ef9d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc104710>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc09c350>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0abe90>, because it is not built.\n",
      "WARNING:tensorflow:From /home/utkarsh/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/utkarsh/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: saved_models/model.01-0.448889-0.55/assets\n",
      "57/57 [==============================] - 47s 831ms/step - loss: 1.0671 - accuracy: 0.4489 - val_loss: 0.9787 - val_accuracy: 0.5467\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.7883 - accuracy: 0.7400WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc13e150>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0d7f10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0ef9d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc104710>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc09c350>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0abe90>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: saved_models/model.02-0.740000-0.77/assets\n",
      "57/57 [==============================] - 46s 804ms/step - loss: 0.7883 - accuracy: 0.7400 - val_loss: 0.6168 - val_accuracy: 0.7667\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.8333WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc13e150>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0d7f10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0ef9d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc104710>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc09c350>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0abe90>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: saved_models/model.03-0.833333-0.82/assets\n",
      "57/57 [==============================] - 51s 893ms/step - loss: 0.5198 - accuracy: 0.8333 - val_loss: 0.4807 - val_accuracy: 0.8200\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 31s 541ms/step - loss: 0.3544 - accuracy: 0.8978 - val_loss: 0.5209 - val_accuracy: 0.7933\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9489WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc13e150>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0d7f10>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0ef9d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc104710>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc09c350>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7fbffc0abe90>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: saved_models/model.05-0.948889-0.85/assets\n",
      "57/57 [==============================] - 47s 831ms/step - loss: 0.2310 - accuracy: 0.9489 - val_loss: 0.4025 - val_accuracy: 0.8533\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 32s 565ms/step - loss: 0.1605 - accuracy: 0.9644 - val_loss: 0.4487 - val_accuracy: 0.8200\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 32s 568ms/step - loss: 0.1142 - accuracy: 0.9733 - val_loss: 0.4364 - val_accuracy: 0.8333\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 30s 518ms/step - loss: 0.0710 - accuracy: 0.9889 - val_loss: 0.4557 - val_accuracy: 0.8533\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 29s 509ms/step - loss: 0.0541 - accuracy: 0.9889 - val_loss: 0.4925 - val_accuracy: 0.8267\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 30s 530ms/step - loss: 0.0463 - accuracy: 0.9933 - val_loss: 0.5122 - val_accuracy: 0.8267\n"
     ]
    }
   ],
   "source": [
    "### ------- Train the model ------- ###\n",
    "# Set an optimizer\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "metric = CategoricalAccuracy('accuracy')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss, \n",
    "    metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x={'input_ids': x_train_enc['input_ids']},\n",
    "    y=y_train_enc_one_hot,\n",
    "    validation_data=({'input_ids': x_test_enc['input_ids']}, y_test_enc_one_hot),\n",
    "    batch_size=bs,\n",
    "    epochs=10,\n",
    "    callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizerFast, TFXLNetMainLayer, TFSequenceSummary\n",
    "from transformers import TFXLNetModel, XLNetConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the pre-trained model to use\n",
    "model_name = 'xlnet-base-cased'\n",
    "\n",
    "# Load transformers config and set number of classes\n",
    "config = XLNetConfig.from_pretrained(model_name)\n",
    "config.num_labels = num_classes\n",
    "\n",
    "# Load XLNet tokenizer\n",
    "tokenizer = XLNetTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=config)\n",
    "\n",
    "# Load the XLNet base model\n",
    "xlnet_model = TFXLNetModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "# Tokenize texts \n",
    "def tokenize(text):\n",
    "    tokenized = tokenizer(text,\n",
    "              padding=True,\n",
    "              truncation=True,\n",
    "              return_tensors='tf',\n",
    "              return_token_type_ids=False,\n",
    "              return_attention_mask=False,\n",
    "              verbose=True)\n",
    "    return tokenized\n",
    "\n",
    "x_train_enc = tokenize(x_train_text)\n",
    "x_test_enc = tokenize(x_test_text)\n",
    "\n",
    "y_train_enc_one_hot = to_categorical(y_train_enc, num_classes=num_classes)\n",
    "y_test_enc_one_hot = to_categorical(y_test_enc, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layers (head-layers)\n",
    "transformer = TFXLNetMainLayer(config, name=\"transformer\")\n",
    "sequence_summary = TFSequenceSummary(config,\n",
    "                                     initializer_range=config.initializer_range, \n",
    "                                     name=\"sequence_summary\")\n",
    "logits_proj = Dense(config.num_labels,\n",
    "                    kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n",
    "                    name=\"logits_proj\")\n",
    "\n",
    "# define model input\n",
    "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "inputs = {'input_ids': input_ids}\n",
    "\n",
    "# forward pass\n",
    "transformer_outputs = xlnet_model(inputs)\n",
    "output = transformer_outputs[0]\n",
    "output = sequence_summary(output)\n",
    "logits = logits_proj(output)\n",
    "\n",
    "# complete a model using TF functional API\n",
    "model = Model(inputs=inputs, outputs=logits, name='Text_Classifier')\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5\n",
    "bs = 16\n",
    "# set callbacks\n",
    "cb = [ModelCheckpoint(filepath='model.{epoch:02d}-{accuracy:02f}-{val_accuracy:.2f}',\n",
    "                      monitor='val_accuracy',\n",
    "                      mode='max',\n",
    "                      save_best_only=True)] # callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ------- Train the model ------- ###\n",
    "# Set an optimizer\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "# Set loss and metrics\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "metric = CategoricalAccuracy('accuracy')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss, \n",
    "    metrics=metric)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    x={'input_ids': x_train_enc['input_ids']},\n",
    "    y=y_train_enc_one_hot,\n",
    "    validation_data=({'input_ids': x_test_enc['input_ids']}, y_test_enc_one_hot),\n",
    "    batch_size=bs,\n",
    "    epochs=2,\n",
    "    callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha6a2vFT0P2-"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1439,
     "status": "ok",
     "timestamp": 1607802370744,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "P9pdFNwLK4EO"
   },
   "outputs": [],
   "source": [
    "def get_preds(x_enc):\n",
    "# inp = tokenizer(x, truncation=True, padding=True)\n",
    "    inp_ids = np.asarray(x_enc['input_ids'])\n",
    "    # att_masks = np.asarray(x_enc['attention_mask'])\n",
    "    preds = np.empty(inp_ids.shape[0], dtype='uint8')\n",
    "    for i in range(inp_ids.shape[0]):\n",
    "        inp_id = inp_ids[i: i+1, :]\n",
    "        # att_mask = att_masks[i: i+1, :]\n",
    "        # pred = model(inputs=inp_id, attention_mask=att_mask)\n",
    "        pred = model(inputs=inp_id)[0]\n",
    "        pred = tf.math.argmax(pred)\n",
    "        pred = pred.numpy()\n",
    "        preds[i] = pred\n",
    "    return preds\n",
    "\n",
    "def get_pred_from_text(txt):\n",
    "    ''' txt: list of texts\n",
    "    '''\n",
    "    pred = tokenize(txt)\n",
    "    pred = get_preds(pred)\n",
    "    return pred\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    ''' y_true, y_pred: type: numpy-array\n",
    "    '''\n",
    "    return sum(y_pred == y_true)/len(y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16839,
     "status": "ok",
     "timestamp": 1607802386184,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "q4zgJ4TjJ2-0",
    "outputId": "290e5e62-4de5-4109-b10e-09b1ee45bc96"
   },
   "outputs": [],
   "source": [
    "# n = 200\n",
    "test = x_train_text\n",
    "y = np.asarray(y_train_enc)\n",
    "\n",
    "y_pred = get_pred_from_text(test)\n",
    "print(get_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19560,
     "status": "ok",
     "timestamp": 1607802388926,
     "user": {
      "displayName": "Utkarsh Jain",
      "photoUrl": "https://lh3.googleusercontent.com/-gplhvKJjZdQ/AAAAAAAAAAI/AAAAAAAAEhA/cT_zmta4k_Q/s64/photo.jpg",
      "userId": "09628245901381158092"
     },
     "user_tz": -330
    },
    "id": "KS4ysEX4NGCd",
    "outputId": "b38fc101-005e-45db-aeea-07481bfd07c7"
   },
   "outputs": [],
   "source": [
    "test = x_test_text\n",
    "y = np.asarray(y_test_enc)\n",
    "\n",
    "y_pred = get_pred_from_text(test)\n",
    "print(get_score(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOzLMiIe1Yz/Yg9B0BGnxcv",
   "collapsed_sections": [
    "-T7DhxxUb4_b",
    "ycJKtmXohZU3",
    "SaQ6xa_eHl3e"
   ],
   "mount_file_id": "1fFu5VqJlYQeBGX-IDXWttd6f8WmnT3Vw",
   "name": "trasformers_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "email_classification",
   "language": "python",
   "name": "email_classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
